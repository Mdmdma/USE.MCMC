# corrplot(cor.m, type = 'lower', diag = FALSE)
library(cvTools)
library(Metrics)
cvdf<-ulex_tfe_frec
k <- 10 #the number of folds
folds <- cvFolds(nrow(cvdf), K=k)
cvdf$holdoutpred <- rep(0,nrow(cvdf))
estim.out<-list()
stat.out<-list()
pred.list<-list()
pred.list_hp<-list()
pred_stack<-crop(rstack, extent( c(-19., -13, 27, 30)))
pred_hp<-crop(rstack, extent( c(-10., 5, 35, 45))) #for the hiberian peninsula
for(i in 1:k){
# i=1
train <- cvdf[folds$subsets[folds$which != i], ] #Set the training set
validation <- cvdf[folds$subsets[folds$which == i], ] #Set the validation set
newmod <- ranger::ranger(formula = FREC ~ bio5+bio6+bio8+bio13+bio14+bio16+bio17+gdd5+gsp+npp,
importance = 'impurity',
min.node.size=1,
data = train)
#var imp
var.imp<-data.frame(as.matrix(newmod$variable.importance))
var.imp$variable<-rownames(var.imp)
colnames(var.imp)[1]<-'importance'
estim.out[[i]]<-var.imp
stat.out[[i]] <- data.frame(Metrics=c("R2", "RMSE") ,
"Value"= c(round(newmod$r.squared,2),
round(sqrt(newmod$prediction.error),2)),
fold=i)
#prediction on tfe
pred.list[[i]] <- round(predict(pred_stack, newmod, type = "response", predict.all=FALSE, na.rm = T, progress = "text", fun = function(model, ...) predict(model, ...)$predictions))
#prediction on hiberian peninsula
pred.list_hp[[i]] <- round(predict(pred_hp, newmod, type = "response", predict.all=FALSE, na.rm = T, progress = "text", fun = function(model, ...) predict(model, ...)$predictions))
}
#response curves
library(pdp)
install.packages("pdp")
#response curves
library(pdp)
newmod
newmod$call
partial(newmod,
pred.var = c("FREC", "gdd5"),
trim.outliers = TRUE, chull = TRUE, parallel = FALSE,
grid.resolution = 30,  paropts = list(.packages = "ranger"))
df<-partial(newmod,
pred.var = c("FREC", "gdd5"),
trim.outliers = TRUE, chull = TRUE, parallel = FALSE,
grid.resolution = 30,  paropts = list(.packages = "ranger"))
head(df)
plot(df$gdd5,df$FREC)
df %>%
group_by(gdd5) %>%
summarise(FREC=mean(FREC))
df %>%
group_by(gdd5) %>%
summarise(FREC=mean(FREC)) %>%
ggplot(aes(gdd5, FREC))+
geom_point()
summary(df)
?partial
df<-partial(newmod,
pred.var = c("FREC", "gdd5"),
trim.outliers = TRUE, chull = TRUE, parallel = FALSE,   type = c( "classification"),
grid.resolution = 30,  paropts = list(.packages = "ranger"))
knitr::opts_chunk$set(echo = TRUE,
dev = 'pdf',
out.width = "100%")
setwd("/home/ddare/working_files/ulex/Ulex_tenerife_markdown/")
# webshot::install_phantomjs()
library(raster)
library(sf)
library(tidyverse)
library(readxl)
library(stringr)
#plot data
# library(mapview)
library(ggplot2)
library(ggpubr)
# library(dismo)
library(viridis)
#analysis
library(ranger)
library(RStoolbox)
#load GBIF
ulex_gbif=read_delim("/home/ddare/working_files/ulex/gbif_occ/0000014-210914110416597.csv") %>%
dplyr::select(species, "decimalLatitude", "decimalLongitude", coordinatePrecision, countryCode, "year")  %>%
filter(!is.na(decimalLatitude)) %>%
filter(!is.na(decimalLongitude)) %>%
filter(decimalLatitude >25 & decimalLatitude< 60) %>%
filter(decimalLongitude >-11 & decimalLongitude< 40) %>% #to ensure to sample only continental Spain
filter(countryCode=="BE" | countryCode=="FR" | countryCode=="DE" | countryCode=="IE" | countryCode=="IT" |     countryCode=="NL" | countryCode=="PT" | countryCode=="ES" | countryCode=="GB"  |countryCode=="CH")
ulex_gbif=st_as_sf(ulex_gbif, coords = c("decimalLongitude", "decimalLatitude"))
st_crs(ulex_gbif) <- 4326
# mapview(ulex_gbif)
#Load Tenerife samples
ulex_tfe=read_xlsx("/home/ddare/working_files/ulex/ULEX_TF_270921.xlsx", sheet = "ULEX_DEPUR_230321_1") %>%
st_as_sf(coords = c("x2", "y2"))
st_crs(ulex_tfe) <- 4326
# mapview(ulex_tfe)
Worldclim<-raster::getData('worldclim', var='bio', res=10)
envData<-crop(Worldclim, extent(-30, 40, 25, 60))
MyMask=envData$bio1
MyMask[!is.na(MyMask)]<-1
plot(MyMask)
rstack<-stack("maskedPreds.tif")
names(rstack)=c("bio5", "bio6", "bio8", "bio13", "bio14", "bio16", "bio17", "gdd5", "gsp", "npp")
rstack_tmp<-crop(rstack, extent(st_bbox(ulex_gbif)[1], st_bbox(ulex_gbif)[3],
st_bbox(ulex_gbif)[2], st_bbox(ulex_gbif)[4]))
ulex_tfe_sub=ulex_tfe %>%
filter(FREC>0)
ulex_tfe_sub=ulex_tfe %>%
filter(FREC>0)
ulex_df=bind_rows(data.frame(raster::extract(rstack, ulex_gbif, df=TRUE),
"source"="GBIF",
country=ulex_gbif$countryCode),
data.frame(raster::extract(rstack, ulex_tfe_sub, df=TRUE),
"source"="TFE",
country="Tenerife"))
ulex_df=na.omit(ulex_df)
ulex_df
View(ulex_df)
x <- c(4, 3, 1.8, 1.1, .6, .3, .02)
y <- c(2, 7, 16, 27, 40, 51, 66)
# df <- data.frame(x,y)
# a <- df$y[1]/b^df$x[1]
plot(x, y)
exp_line <- function(x1, x2, y1, y2) {
gradient  <- (log(y2) - log(y1)) / (x2 - x1)
intercept <- log(y1) - gradient * x1
cat("y = ", exp(gradient), "^x * ", exp(intercept), "\n", sep = "")
}
exp_line(x[1], x[2], y[1], y[2])
exp_line(x[4], x[5], y[4], y[5])
draw_exp_line <- function(x1, x2, y1, y2, col) {
gradient  <- (log(y2) - log(y1)) / (x2 - x1)
intercept <- log(y1) - gradient * x1
x <- seq(0, 5, 0.1)
lines(x, exp(gradient)^x * exp(intercept), col = col)
}
plot(x, y)
draw_exp_line(x[2], x[1], y[2], y[1], "red")
draw_exp_line(x[1], x[2], y[1], y[2], "red")
plot(x, y)
draw_exp_line(x[1], x[2], y[1], y[2], "red")
exp_line(x[1], x[2], y[1], y[2])
intercept
intercept
debug(draw_exp_line)
draw_exp_line(x[1], x[2], y[1], y[2], "red")
draw_exp_line(x[1], x[2], y[1], y[2], "red")
intercept
y2
y1
exp(intercept)
plot(x, y)
# Directory containing the census polygons
P_census="/home/ddare/working_files/GLW_TS/Census_2000_2020/"
# Continent
P_continent = "AFR"
#define species
P_species = "CTAL"
# First year from which to use training data. There might not be predictor variables to match historical livestock counts.
P_FirstYear = 2005
# Last year from which to use training data. There might not be predictor variables to match all livestock counts.
P_LastYear = 2008
P_ExtentFile = "/home/ddare/working_files/GLW_TS/crosstables/ExtentsGLW.csv"
P_MaxArea = 900000000
###################################################################
## Data pre-processing of response variable
###################################################################
sf::sf_use_s2(FALSE)
# Select continent
country_tab <- read_csv("/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv") %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
library(dplyr)
# Select training polygons by species and subset accordingly to time frame of interest
lsGeom <- list.files(paste0(P_census, P_species), pattern=".shp$", full.names = TRUE)
lsGeom <- unique (grep(paste(P_FirstYear:P_LastYear,collapse="|"), lsGeom, value=TRUE))
lsGeom <- lapply(lsGeom, function(x){st_read(x) %>% dplyr::filter(ISO3%in% country_tab)%>% dplyr::filter(N0>=0)})
library(sf)
lsGeom <- lapply(lsGeom, function(x){st_read(x) %>% dplyr::filter(ISO3%in% country_tab)%>% dplyr::filter(N0>=0)})
# Select continent
country_tab <- read_csv("/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv") %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
library(readr)
# Select continent
country_tab <- read_csv("/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv") %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
# Select training polygons by species and subset accordingly to time frame of interest
lsGeom <- list.files(paste0(P_census, P_species), pattern=".shp$", full.names = TRUE)
# Directory containing the census polygons
P_census="/home/ddare/working_files/GLW_TS/Census_2000_2020/"
# Continent
P_continent = "AFR"
#define species
P_species = "CTAL"
# First year from which to use training data. There might not be predictor variables to match historical livestock counts.
P_FirstYear = 2005
# Last year from which to use training data. There might not be predictor variables to match all livestock counts.
P_LastYear = 2008
P_ExtentFile = "/home/ddare/working_files/GLW_TS/crosstables/ExtentsGLW.csv"
P_MaxArea = 900000000
###################################################################
## Data pre-processing of response variable
###################################################################
sf::sf_use_s2(FALSE)
# Select continent
country_tab <- read_csv("/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv") %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
# Continent
P_Continent = "AFR"
# Select continent
country_tab <- read_csv("/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv") %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
# Select training polygons by species and subset accordingly to time frame of interest
lsGeom <- list.files(paste0(P_census, P_species), pattern=".shp$", full.names = TRUE)
lsGeom <- unique (grep(paste(P_FirstYear:P_LastYear,collapse="|"), lsGeom, value=TRUE))
lsGeom <- lapply(lsGeom, function(x){st_read(x) %>% dplyr::filter(ISO3%in% country_tab)%>% dplyr::filter(N0>=0)})
names(lsGeom) <- as.character(P_FirstYear:P_LastYear)
# Reduce extent
ext <- read_csv(P_ExtentFile)
EA_CRS <- ext[ext$CONTINENT_GLW == P_Continent,"EA_PROJECTION"]
EA_CRS <- EA_CRS$EA_PROJECTION
ext = extent(as.numeric(ext[ext$CONTINENT_GLW == P_Continent,c("XMIN", "XMAX", "YMIN", "YMAX")]))
ext = raster::extent(as.numeric(ext[ext$CONTINENT_GLW == P_Continent,c("XMIN", "XMAX", "YMIN", "YMAX")]))
lsGeom <- lapply(lsGeom, function(x){st_crop(x, ext)})
#Transform to equal area projection
lsGeom_ea <- lapply(lsGeom, function(x){st_transform(x, crs = EA_CRS )})
lsGeom_ea <-lapply(lsGeom_ea, function(x) { x$AREA_TO <- as.numeric(st_area(x))/10^6;return(x)}) #Calculate total administrative unit area in square kilometers
# Remove units larger than the specified maximum
lsGeom_ea <-lapply(lsGeom_ea, function(x) {x %>% filter(AREA_TO < P_MaxArea)} )
# Need to cast to polygon since exact_extract cannot handle multipolygon geometries
lsGeom_ea_poly <- lapply(lsGeom_ea, function(x){st_cast(x, "POLYGON")})
myRaster <- raster("/home/ddare/Downloads/GLWLandmask_ea.tif")
library(raster)
myRaster <- raster("/home/ddare/Downloads/GLWLandmask_ea.tif")
# Calculate suitable areas
extractval <- lapply(lsGeom_ea_poly, function(x){exact_extract(myRaster, x, fun=weighted.mean, na.rm=TRUE)}) # We get NA values in some cases here
library(exactextractr)
# Calculate suitable areas
extractval <- lapply(lsGeom_ea_poly, function(x){exact_extract(myRaster, x, fun=weighted.mean, na.rm=TRUE)}) # We get NA values in some cases here
lsGeom_ea_poly<-lapply(lsGeom_ea_poly, function(x) { x$AREA_PO <- as.numeric(st_area(x))/10^6;return(x)}) # Calculate individual polygon areas in square kilometers
lsGeom_ea_poly
extractval
lsGeom_ea_poly$`2005`$AREA_PO * extractval$`2005`
length(extractval)
lsGeom_ea_poly[[1]]
lsGeom_ea_poly[[1]]$AREA_PO
mapply(function(X,Y) {
lapply(1:1, function(i){ X[[i]]$SAREA_PO <- as.numeric(X[[i]]$AREA_PO * Y[[i]] );return(X)})
}, X=lsGeom_ea_poly, Y=extractval)
lapply(1:1, function(i){ X[[i]]$SAREA_PO <- as.numeric(X[i]$AREA_PO * Y[i] );return(X)})
X=lsGeom_ea_poly; Y=extractval
X[[1]]
# Creating a list
A = list(c(1, 2, 3, 4))
# Creating another list
B = list(c(2, 5, 1, 6))
# Applying mapply()
result = mapply(sum, A, B)
result
# Creating a list
A = list(c(1, 2, 3, 4), c(1, 2, 3, 4))
# Creating another list
B = list(c(2, 5, 1, 6), c(2, 5, 1, 6))
# Applying mapply()
result = mapply(sum, A, B)
result
mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
a<-lsGeom_ea_poly$`2005`$AREA_PO * extractval$`2005`
b<-mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
all_equal(a, b$`2005`)
all.equal(a, b$`2005`)
b<-mapply(function(x, y){x$SAREA_PO <- x$AREA_PO*y; return(x)}, lsGeom_ea_poly, extractval)
b
b<-mapply(function(x, y){x$SAREA_PO <- x$AREA_PO*y}, lsGeom_ea_poly, extractval)
b
SAREA_PO <-<-mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
SAREA_PO <-mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
SAREA_PO
SAREA_PO
SAREA_PO <-mapply(function(x, y){data.frame("SAREA_PO"<-x$AREA_PO*y)}, lsGeom_ea_poly, extractval)
SAREA_PO
class(SAREA_PO$`2005.X.SAREA_PO.....x.AREA_PO...y`)
SAREA_PO <-mapply(function(x, y){data.frame(x$AREA_PO*y)}, lsGeom_ea_poly, extractval)
SAREA_PO
lapply(lsGeom_ea_poly, function(x) append(x, "SAREA_PO"))
mapply(function(x, y){x$SAREA_PO<-x$AREA_PO*y}, lsGeom_ea_poly, extractval)
?append
SAREA_PO <-mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
append(lsGeom_ea_poly, SAREA_PO)
lapply(lsGeom_ea_poly, function(x) {append(x, SAREA_PO[[x]])})
lapply(lsGeom_ea_poly, function(x) {append(x, SAREA_PO[x])})
mapply(function(x, y){append(x, y)}, lsGeom_ea_poly, SAREA_PO)
mapply(function(x, y){cbind(x, y)}, lsGeom_ea_poly, SAREA_PO)
mapply(function(x, y) "[<-"(x, "SAREA_PO", value = y) ,
lsGeom_ea_poly, SAREA_PO, SIMPLIFY = FALSE)
lsGeom_ea_poly <-  mapply(function(x, y) "[<-"(x, "SAREA_PO", value = y), lsGeom_ea_poly, SAREA_PO, SIMPLIFY = FALSE)
lsGeom_ea_poly
names(lsGeom_ea_poly$`2005`)
P_species = "CTAL"
lsGeom_ea_poly$`2005`$LEVEL
# Create unique ID polygon
paste0(P_species, lsGeom_ea_poly$`2005`$YEAR, lsGeom_ea_poly$`2005`$ISO3, lsGeom_ea_poly$`2005`$LEVEL, "_", 1:nrow(lsGeom_ea_poly$`2005`) )
# Create unique ID polygon
lsGeom_ea_poly <-lapply(lsGeom_ea_poly, function(x) { x$ID_GLW<- paste0(P_species, x$YEAR, x$ISO3, x$LEVEL, "_", 1:nrow(lsGeom_ea_poly$`2005`) );return(x)}) #Calculate total administrative unit area in square kilometers
# Create unique ID polygon
lsGeom_ea_poly <-lapply(lsGeom_ea_poly, function(x) { x$ID_GLW<- paste0(P_species, x$YEAR, x$ISO3, x$LEVEL, "_", 1:nrow(x) );return(x)}) #Calculate total administrative unit area in square kilometers
#Save the livpop data in csv-file
livpop <- lapply(lsGeom_ea_poly, st_drop_geometry)
livpop
livpop <- do.call(rbind.data.frame, livpop)
#---- General information of the run ----
# Asbsolute path of GLW
P_GenPath = "/home/ddare/working_files/GLW_TS/"
# General name of the run
P_myOutName = "africa"
# General folder of the run
P_myOutData = paste(P_GenPath,"outputs_2022/",P_myOutName,sep="")
# Temp folders to be used as scratch (local)
P_ProcessingFolder = "3_Processing"
# Temp folders to be used as outputs
P_OutputFolder = "4_Outputs"
# Temp folders to be used for postprocessing outputs
P_PostProFolder = "5_Postprocessing"
#---- Response variable ----
# Directory containing the census polygons
P_census="/home/ddare/working_files/GLW_TS/Census_2000_2020/"
# Continent
P_Continent = "AFR"
# Define species
P_species = "CTAL"
# First year from which to use training data. There might not be predictor variables to match historical livestock counts.
P_FirstYear = 2005
# Last year from which to use training data. There might not be predictor variables to match all livestock counts.
P_LastYear = 2010
# Maximum area (square kilometers) for administrative units to be used by the model
P_MaxArea = 900000000   #10000
# Select continent
country_tab <- read_csv(P_subdiv) %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
#---- Run GLW ----
source("/home/ddare/working_files/GLW_TS/GLW_code_2022/2022/GLW_1_Libraries.r")
# Select continent
country_tab <- read_csv(P_subdiv) %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
?read.csv
?read_csv
library(readr)
#library(rtsa)
library(colorRamps)
library(foreach)
library(doParallel)
# Select continent
country_tab <- read_csv(P_subdiv) %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
# File containing the supranational division for each continent
P_subdiv =  "/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv"
#---- Response variable ----
# Directory containing the census polygons
P_census="/home/ddare/working_files/GLW_TS/Census_2000_2020/"
# Continent
P_Continent = "AFR"
# Define species
P_species = "CTAL"
# First year from which to use training data. There might not be predictor variables to match historical livestock counts.
P_FirstYear = 2005
# Last year from which to use training data. There might not be predictor variables to match all livestock counts.
P_LastYear = 2010
# Maximum area (square kilometers) for administrative units to be used by the model
P_MaxArea = 900000000   #10000
#---- Predictors ----
# File containing the geographical extents for each continent
P_ExtentFile =  "/home/ddare/working_files/GLW_TS/crosstables/ExtentsGLW.csv"
# File containing the supranational division for each continent
P_subdiv =  "/home/ddare/working_files/GLW_TS/crosstables/countries_subdivisions.csv"
# Directory containing the area raster files
P_rast_area="/home/ddare/working_files/GLW_TS/area/wdareakm.tif"
# Directory containing the predictor raster files
P_Predictors = "/home/ddare/working_files/GLW_TS/area/wdareakm.tif"
# Select continent
country_tab <- read_csv(P_subdiv) %>%
filter(Continent_GLW==P_Continent) %>%
select(ISO3) %>%
pull()
k=1
length(P_species)
# Select training polygons by species and subset accordingly to time frame of interest
lsGeom <- list.files(paste0(P_census, P_species[k]), pattern=".shp$", full.names = TRUE)
lsGeom <- unique (grep(paste(P_FirstYear:P_LastYear,collapse="|"), lsGeom, value=TRUE))
lsGeom <- lapply(lsGeom, function(x){st_read(x) %>% dplyr::filter(ISO3%in% country_tab)%>% dplyr::filter(N0>=0)})
names(lsGeom) <- as.character(P_FirstYear:P_LastYear)
# Last year from which to use training data. There might not be predictor variables to match all livestock counts.
P_LastYear = 2008
# Select training polygons by species and subset accordingly to time frame of interest
lsGeom <- list.files(paste0(P_census, P_species[k]), pattern=".shp$", full.names = TRUE)
lsGeom <- unique (grep(paste(P_FirstYear:P_LastYear,collapse="|"), lsGeom, value=TRUE))
lsGeom <- lapply(lsGeom, function(x){st_read(x) %>% dplyr::filter(ISO3%in% country_tab)%>% dplyr::filter(N0>=0)})
names(lsGeom) <- as.character(P_FirstYear:P_LastYear)
# Reduce extent
ext <- read_csv(P_ExtentFile)
EA_CRS <- ext[ext$CONTINENT_GLW == P_Continent,"EA_PROJECTION"]
EA_CRS <- EA_CRS$EA_PROJECTION
ext = raster::extent(as.numeric(ext[ext$CONTINENT_GLW == P_Continent,c("XMIN", "XMAX", "YMIN", "YMAX")]))
lsGeom <- lapply(lsGeom, function(x){st_crop(x, ext)})
###################################################################
## Data pre-processing of response variable
###################################################################
sf::sf_use_s2(FALSE)
lsGeom <- lapply(lsGeom, function(x){st_crop(x, ext)})
#Transform to equal area projection
lsGeom_ea <- lapply(lsGeom, function(x){st_transform(x, crs = EA_CRS )})
lsGeom_ea <-lapply(lsGeom_ea, function(x) { x$AREA_TO <- as.numeric(st_area(x))/10^6;return(x)}) #Calculate total administrative unit area in square kilometers
dim(lsGeom_ea)
dim(lsGeom_ea$`2005`)
# Remove units larger than the specified maximum
lsGeom_ea <-lapply(lsGeom_ea, function(x) {x %>% filter(AREA_TO < P_MaxArea)} )
dim(lsGeom_ea$`2005`)
# Need to cast to polygon since exact_extract cannot handle multipolygon geometries
lsGeom_ea_poly <- lapply(lsGeom_ea, function(x){st_cast(x, "POLYGON")})
myRaster<-raster(P_rast_area)
# Calculate suitable areas
extractval <- lapply(lsGeom_ea_poly, function(x){exact_extract(myRaster, x, fun=weighted.mean, na.rm=TRUE)}) # We get NA values in some cases here
lsGeom_ea_poly<-lapply(lsGeom_ea_poly, function(x) { x$AREA_PO <- as.numeric(st_area(x))/10^6;return(x)}) # Calculate individual polygon areas in square kilometers
SAREA_PO <-mapply(function(x, y){x$AREA_PO*y}, lsGeom_ea_poly, extractval)
lsGeom_ea_poly <-  mapply(function(x, y) "[<-"(x, "SAREA_PO", value = y), lsGeom_ea_poly, SAREA_PO, SIMPLIFY = FALSE)
# Create unique ID polygon
lsGeom_ea_poly <-lapply(lsGeom_ea_poly, function(x) { x$ID_GLW<- paste0(P_species[k], x$YEAR, x$ISO3, x$LEVEL, "_", 1:nrow(x) );return(x)}) #Calculate total administrative unit area in square kilometers
#------USE-----------------
setwd("/home/ddare/GitHub/USE/")
# token "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"
# token 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
library(devtools)
library(roxygen2)
library(usethat)
# devtools::create("yourPkg")
# devtools::install_github("mattmar/dynamAedes")
devtools::load_all(".") # Working directory should be in the package directory
#update documentation
devtools::document()
remove.packages("USE")
# devtools::create("yourPkg")
# devtools::install_github("mattmar/dynamAedes")
devtools::load_all(".") # Working directory should be in the package directory
#update documentation
devtools::document()
library(spelling)
spell_check()
# devtools::create("yourPkg")
# devtools::install_github("mattmar/dynamAedes")
devtools::load_all(".") # Working directory should be in the package directory
#update documentation
devtools::document()
spell_check()
# devtools::create("yourPkg")
# devtools::install_github("mattmar/dynamAedes")
devtools::load_all(".") # Working directory should be in the package directory
#update documentation
devtools::document()
spell_check()
#check everything is ok for the CRAN
check()
#website
# devtools::install_github('r-lib/pkgdown')
library(pkgdown)
# Run to build the website
pkgdown::build_site("/home/ddare/GitHub/USE/", install = FALSE,
examples = FALSE)
library(parallel)
library(tidyverse)
# Run to build the website
pkgdown::build_site("/home/ddare/GitHub/USE/", install = FALSE,
examples = FALSE)
Sys.setlocale("LC_ALL", "English")
library(USE)
library(raster)
library(virtualspecies)
library(RStoolbox)
library(data.table)
library(sf)
myCRS<-"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
#install package
devtools::install(".")
# Run to build the website
pkgdown::build_site("/home/ddare/GitHub/USE/", install = FALSE,
examples = FALSE)
# if you change the readme description .Rmd, then run
use_readme_rmd(open = rlang::is_interactive())
# if you change the readme description .Rmd, then run
# use_readme_rmd(open = rlang::is_interactive())
devtools::build_readme()
# Run to build the website
pkgdown::build_site("/home/ddare/GitHub/USE/", install = FALSE,
examples = FALSE)
